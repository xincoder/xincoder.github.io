[{"authors":["**Xin Li**","Xiaowen Ying","Mooi Choo Chuah"],"categories":null,"content":"","date":1564297200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564297200,"objectID":"3b85241a1e3c2affab4a858809f437d8","permalink":"http://xincoder.github.io/publication/2019itsc_grip/","publishdate":"2019-07-28T00:00:00-07:00","relpermalink":"/publication/2019itsc_grip/","section":"publication","summary":"","tags":null,"title":"GRIP: Graph-based Interaction-aware Trajectory Prediction","type":"publication"},{"authors":["**Xin Li**","Shuai Zhang","Bolan Jiang","Yingyong Qi","Mooi Choo Chuah","Ning Bi"],"categories":null,"content":"Please refer to our project demo page for visualized results.\n","date":1546934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546934400,"objectID":"8e7bda27028ddecd6cc9010689f17ad8","permalink":"http://xincoder.github.io/publication/2019wacv_dac/","publishdate":"2019-01-08T00:00:00-08:00","relpermalink":"/publication/2019wacv_dac/","section":"publication","summary":"Please refer to our project demo page for visualized results.","tags":null,"title":"DAC: Data-free Automatic Acceleration of Convolutional Networks","type":"publication"},{"authors":null,"categories":null,"content":"Programming Language:\n Python  This is a prototype of our \u0026ldquo;Data-free convolutional network acceleration\u0026rdquo; scheme. In this demo, we demonstrate the performance of our scheme in the task of multi-person pose estimation model and object detection. Please refer to our paper for more results and details.\nThe following video shows a demo of our scheme.\n Published paper:\n Xin Li, Shuai Zhang, Bolan Jiang, Yingyong Qi, Mooi Choo Chuah, Ning Bi (2019). DAC: Data-free Automatic Acceleration of Convolutional Networks. IEEE Winter Conference on Applications of Computer Vision (WACV). 2019.  ","date":1541404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541404800,"objectID":"67c2daa38a0abf67e2ea64a0c274f51e","permalink":"http://xincoder.github.io/project/research_dac/","publishdate":"2018-11-05T00:00:00-08:00","relpermalink":"/project/research_dac/","section":"project","summary":"This is demo of our DAC paper.","tags":["Research Project","Python"],"title":"Data-free CNN acceleration [Research]","type":"project"},{"authors":["Xiaowen Ying","**Xin Li**","Mooi Choo Chuah"],"categories":null,"content":"","date":1538377200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538377200,"objectID":"f9a529314a8ecbbf3b193158e1810f02","permalink":"http://xincoder.github.io/publication/2018icmla_liveface/","publishdate":"2018-10-01T00:00:00-07:00","relpermalink":"/publication/2018icmla_liveface/","section":"publication","summary":"","tags":null,"title":"LiveFace: A Multi-Task CNN for Fast Face-Authentication","type":"publication"},{"authors":null,"categories":null,"content":" Vehicle Detection Project Programming Language:\n Python  The goals of this project are the following:\n Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. Note: for those first two steps don\u0026rsquo;t forget to normalize your features and randomize a selection for training and testing. Implement a sliding-window technique and use your trained classifier to search for vehicles in images. Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles. Estimate a bounding box for vehicles detected.  1. Feature Extraction I extract the binned color, histogram of color and HOG as my feature and trained a SVC classifier using the concatenated feature. The following shows the parameters that I used:\n         Feature Setting   Color Space YUV   orient 9   pix_per_cell 8   cell_per_block 2   hog_channel \u0026lsquo;ALL\u0026rsquo;   spatial_size (16,16)   hist_bins 16    The SVC classifier is used to predict a give image is a are or not. The following image shows a positive and a negative sample. The computed histogram of color is shown as follow: The below figure shows the visualized HOG featgure. I applied sliding window on the bottom half part of the image. For each window, the chosen features are extracted and passed to the pre-trained SVC classifier to predict if the current location is a car or not. Then, we generate a heat map based on the detected results and filter low value out. The following shows the visualized detected results.\n          Detected BBoxes Heat Map Final Detection                                  2. Make it smooth. Till now, the pipline works on single frame. However, there is no relationship between two continuous frames and the detected results are independent. Thus, I came up with an idea that sonsidering several continuous frames to make the detected results more smoothly. I record 3 latest heatmap and then sum them together before doing threshold. Using this way, the detected results become much more stable. Please refer the demo video.\n3. Potential Issue. The parameters of the current pipline are manually chosen, the trained model and chosen parameters may not work for all videos.\nPotential solution: using deep learning object detection models, e.g. SSD.\n","date":1535871600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535871600,"objectID":"00aa7f6e1861eed97b2d8aa8fd751a8f","permalink":"http://xincoder.github.io/project/selfdriving_vehicle_detection/","publishdate":"2018-09-02T00:00:00-07:00","relpermalink":"/project/selfdriving_vehicle_detection/","section":"project","summary":"Detect vehicles on road from images.","tags":["Self Driving","Python"],"title":"Vehicle Detection[Self Driving]","type":"project"},{"authors":null,"categories":null,"content":" Advanced Lane Finding Project Programming Language:\n Python   The goals of this project are the following:\n Compute the camera calibration matrix and distortion coefficients given a set of chessboard images. Apply a distortion correction to raw images. Use color transforms, gradients, etc., to create a thresholded binary image. Apply a perspective transform to rectify binary image (\u0026ldquo;birds-eye view\u0026rdquo;). Detect lane pixels and fit to find the lane boundary. Determine the curvature of the lane and vehicle position with respect to center. Warp the detected lane boundaries back onto the original image. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.  A. Pipeline. My pipline consists of 7 steps as follows:\n Step 1: Camera Calibration Step 2: Distortion correction Step 3: Detect lines based on color and gradient Step 4: Perspective transform Step 5: Detect lane lines Step 6: Determine the lane curvature Step 7: Determine vehicle offset from center  B. Visualized Results 1. Camera calibration and Distortion correction: Image distortion occurs when a camera looks at 3D objects in the real world and transforms them into a 2D image; this transformation isnâ€™t perfect. Distortion changes what the shape and size of these 3D objects appear to be. The following is a sample of the processed results:\n         Input image After Calibration        One can see that the lines on the left side of the image become straight after distortion.\n2: Detect lines based on color and gradient: Detect lines of the current lane based on color and gradient.\n I first convert the image from RGB color space to HLS color space. The following shows the HLS color space.            H channel L channel S channel          Then, I do color selection on S channel and x gradient on L channel. The following shows the combined result:           Input Imgae Color_Gradient Result         After that, I do open operation on the previous result to remove noise. One can see that the tiny noise is remove.           Input Imgae Open Operation        4. Perspective transform: To detect lines, I convert the image to birdeye view. I chose the following source and destination points:\n         Source Points Destination Points   200, 200 566, 470   980, 200 714, 470   980, 700 1055, 680   200, 700 253, 680    The following shows a sample. It is clear that two lines are parallel.\n         Input Imgae Open Operation        5. Detect lane lines:\n First, I calculate histogram on vertical direction.           Input Imgae Histogram         Then, I detect lines and compute their polynomial functions.           Input Imgae Detect Lines        6. Visualize final result:\n         Input Imgae Detect Lines        Potential improvements  This pipeline may be not work perfectly if the line is not very clear. Thus, merging the results generated using different color space will improve the performance. The perspective transform matrix may be not exactly same for different camera. Thus, we need a smarter way to calculate it automatically.  ","date":1534748400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534748400,"objectID":"f9776065d062e44aec660ec02846f485","permalink":"http://xincoder.github.io/project/selfdriving_adv_findline/","publishdate":"2018-08-20T00:00:00-07:00","relpermalink":"/project/selfdriving_adv_findline/","section":"project","summary":"Detect lane pixels and find the lane boundary from images.","tags":["Self Driving","Python"],"title":"Detect Lane [Self Driving]","type":"project"},{"authors":null,"categories":null,"content":" Drive in a simulator Programming Language:\n Python   The goals of this project are the following:\n Use the simulator to collect data of good driving behavior Build, a convolution neural network in Keras that predicts steering angles from images Train and validate the model with a training and validation set Test that the model successfully drives around track one without leaving the road Summarize the results with a written report  Section 1: Model Architecture and Training Strategy 1. Model architecture The following figure shows the architecture of my model.\n First, I normalized all pixels of an input frame to [-1, 1]. Second, I use Cropping2D to remove sky and the hood from every single frame. Third, 3 (5x5) convolutional layers are used to extract visual features from input frame. All convolutional layers have a RELU activation function to introduce nonlinearity. Each convolutional layer is followed by a MaxPooling2D layer to reduce the size of feature maps. Then, 1 flatten layer [model.py line 39] convert the feature maps to a vector and two fully connect layers are used to predict the final result.  Predicting the angle of the steering wheel is a regression problem. Thus I used \u0026ldquo;mse\u0026rdquo; loss function.\n2. Attempts to reduce overfitting in the model  Dropout: The model contains dropout layers in order to reduce overfitting. Data collection: On both tracks, I drive the car Clockwise (2 loops) and counterclockwise (1 loop) and save data. Training: The model was trained and validated on different data sets to ensure that the model was not overfitting. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track. Data Augmentation: Every training frame has 50% probability to be horizontally flipped during training.  3. Model parameter tuning The model used an adam optimizer, so the learning rate was not tuned manually.\n4. Appropriate training data Training data was chosen to keep the vehicle driving on the road. I used a combination of center lane driving, recovering from the left and right sides of the road. Based on the ground truth:\n If the current angle is 0, then the correction value for data from the left and right camera is set to a small value. Otherwise, a larger correction value is used. Using this way can avoid the car from performing as a snake.  All data are randomly split into two subsets: 80% for training and 20% for validation. The model normalized input to [-1, 1] and crop out the top and the bottom parts to remove meaningless context.\nThe following two figures are the original input and the result after cropped.\n         Original Input Cropped Input        The following shows the number of collected data. As you can see that, after using data from all 3 cameras and randomly horizontally flip frames, the distributation of the data is more balance.\n         Raw Data Use all 3 cameras \u0026amp; Data Augmentation        Section 3: Training History and Model Visualization This figure shows the loss history during training. I also visulized the trained model and tried to understand what features the model learned. Section 4: Conclusion and Future work In this project, I proposed a Convolutional Network to predict the angle of the steering wheel running in an emulator.\nThe trained model successful negatives the car. Even in the second track (more challenging than the first one), the model still performs great.\nOne of the possible drawback of the current model is taht it was only trained on the data collected from 2 tracks. Thus, it may cannot perform perfectly in other unseen scenario.\nThus, in the future, I would like to train the model using more data collected from different scens.\n","date":1533279600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533279600,"objectID":"32a6bd2a6cf5aef348fcd5aa6405866f","permalink":"http://xincoder.github.io/project/selfdriving_drive_emulator/","publishdate":"2018-08-03T00:00:00-07:00","relpermalink":"/project/selfdriving_drive_emulator/","section":"project","summary":"Train a model to predict steering angles.","tags":["Self Driving","Python"],"title":"Drive in An Emulator [Self Driving]","type":"project"},{"authors":null,"categories":null,"content":" Traffic Sign Recognition Programming Language:\n Python  The goals of this project are the following:\n Load the data set (see below for links to the project data set) Explore, summarize and visualize the data set Design, train and test a model architecture Use the model to make predictions on new images Analyze the softmax probabilities of the new images Summarize the results with a written report  Step1: Data Set Summary \u0026amp; Exploration 1. A basic summary of the data set. I use Pickle data load function to read our dataset, and then use basic python function to analyze the dataset. In total, we have:\n Training samples: 34,799 Validation samples: 4,410 Testing samples: 12,630 Each sample (image) has (32, 32, 3) shape. We have 43 unique classes/labels:                ClassId SignName ClassId SignName ClassId SignName   0 Speed limit (20km/h) 1 Speed limit (30km/h) 2 Speed limit (50km/h)   3 Speed limit (60km/h) 4 Speed limit (70km/h) 5 Speed limit (80km/h)   6 End of speed limit (80km/h) 7 Speed limit (100km/h) 8 Speed limit (120km/h)   9 No passing 10 No passing for vehicles over 3.5 metric tons 11 Right-of-way at the next intersection   12 Priority road 13 Yield 14 Stop   15 No vehicles 16 Vehicles over 3.5 metric tons prohibited 17 No entry   18 General caution 19 Dangerous curve to the left 20 Dangerous curve to the right   21 Double curve 22 Bumpy road 23 Slippery road   24 Road narrows on the right 25 Road work 26 Traffic signals   27 Pedestrians 28 Children crossing 29 Bicycles crossing   30 Beware of ice/snow 31 Wild animals crossing 32 End of all speed and passing limits   33 Turn right ahead 34 Turn left ahead 35 Ahead only   36 Go straight or right 37 Go straight or left 38 Keep right   39 Keep left 40 Roundabout mandatory 41 End of no passing   42 End of no passing by vehicles over 3.5 metric tons        2. Here is an exploratory visualization of the data set. Step2: Design and Test a Model Architecture 1. Image data preprocessing I use two methods to do data augmentation (preprocessing).\n Randomly rotation: all training images have 50% probability to be rotated within (-30, 30) angle. Rotation is needed considering that in daily life, the camera on a self-driving car may capture a traffice sign with a radom angle. Thus, by doing this help the model has the capability of handling such an application scenario. Randomly crop: all training images have 50% probability to be cropped from (0.8, 1) of width and height. After being cropped, the images will be resized back to 32x32. Cropping is necessary because the captured traffic signs have variant sizes. Thus, using cropping adds more training data. In addition, randomly cropping provides some smaples with width and height shifts.  Here are some processed images: 2. Model architecture My model consisted of the following layers:\n   Layer Input_Size Output_size Description     Convolution(5x5) 32x32x3 28x28x64 (1x1) stride, VALID padding, Relu Activation   Convolution(3x3) 28x28x64 28x28x64 (1x1) stride, SAME padding, Relu Activation   Pooling 28x28x64 14x14x6    Convolution(5x5) 14x14x64 10x10x128 (1x1) stride, VALID padding, Relu Activation   Convolution(3x3) 10x10x128 10x10x128 (1x1) stride, SAME padding, Relu Activation   Pooling 10x10x128 5x5x12    Convolution(3x3) 5x5x128 3x3x256 (1x1) stride, VALID padding, Relu Activation   Pooling 3x3x256 1x1x25    Flatten 1x1x256 256    Fully Connected 256 128 Relu Activation   Dropout layer      Fully Connected 128 43 Softmax Activation    3. Training parameters I use the following experimental settings to train my model:\n Optimizer: Adam Initialial Learning Rate: 0.001 Batch Size: 128 Training Epochs: 100 Loss function: cross entropy.  4. Accuracy During training, I tracked the validation accuracy and only saved the weights achieved the highest validation accuracy. Within 100 training epochs, my architecture achieves the highest val_acc at 84 epoch. (All details about training process can be found in Cell [10])\n The accuracy on training set: 99.9% The accuracy on validation set: 96.2% The accuracy on testing set: 93.4%  The followings are the reasons why I chose such a model for this task:\n In the first convolutional laeyrs, the model focuses on detecting fundamental features, e.g. lines, shapes, or textures. Thus, at the first two blocks (4 convolutional layers), I used two continuous convolutional layers to make sure that the model generates more useful basic features. Following that, a block with one convolutional layer is used to generate semantic information, e.g. arrows, circles, and etc.. Comparing to the basic features, we have more semantic features, thus, more filters were used in this layer. A fully connected layer is used to convert a feature map into a verctor for classification. One Dropout layer is used to avoid overfitting.  Test a Model on New Images 1. Unseen data during traing Here are five German traffic signs that I found on the web: 2. Prediction results on these new traffic signs. Here are the results of the prediction:\n          Image Ground Truth Prediction Predicted Correctly   Go straight or right Go straight or right True   Speed limit (50km/h) Speed limit (50km/h) True   Stop Yield False   Road work Road work True   Turn right ahead Turn right ahead True    The model was able to correctly guess 4 of the 5 traffic signs, which gives an accuracy of 80%. We can see it drops a lot comparing to the accuracy on our testing dataset. The reason is three-fold:\n The training dataset is not balance. For example, class 2 (Speed limit (50km/h)) has 2010 training samples, while class 14 (Stop) only consistes of 330 training samples. The downloaded images are much clearer than the images in our dataset, which may add more noise in the images. 5 images are not enough to evaluate the performance of a trained model.  3. Prediction details For the first image, the model is 100% sure that it is a \u0026ldquo;Go straight or right\u0026rdquo;. It is correctly predict this image.\n         Probability Prediction   1.0000000 36 (Go straight or right)   2.8858561e-16 3 (Speed limit (60km/h))   2.1692284e-31 0 (Speed limit (20km/h))   7.6920753e-32 20 (Dangerous curve to the right)   2.2140123e-32 28 (Children crossing)    For the second image, the model is 100% sure that it is a \u0026ldquo;Speed limit (50km/h)\u0026rdquo;. It is correctly predict this image.\n         Probability Prediction   9.9999976e-01 2 (Speed limit (50km/h))   2.8747601e-07 5 (Speed limit (80km/h))   2.2901984e-10 1 (Speed limit (30km/h))   2.2646303e-18 3 (Speed limit (60km/h))   4.5397839e-35 6 (End of speed limit (80km/h))    For the third image, the model is 100% sure that it is a \u0026ldquo;Yield\u0026rdquo;, while the ground truth is \u0026ldquo;Stop\u0026rdquo;. There are only 690 \u0026ldquo;Stop\u0026rdquo; training samples in our dataset, while \u0026ldquo;Yield\u0026rdquo; has 1290 training images. This is caused by our un-balance training data.\n         Probability Prediction   1.0000000e+00 13 (Yield)   4.3748559e-12 28 (Children crossing)   1.5492816e-12 1 (Speed limit (30km/h))   1.0894177e-12 38 (Keep right)   6.2159755e-13 2 (Speed limit (50km/h))    For the fourth image, the model is 100% sure that it is a \u0026ldquo;Road work\u0026rdquo;. It is correctly predict this image.\n         Probability Prediction   1.0000000e+00 25   0.0000000e+00 0 (Speed limit (20km/h))   0.0000000e+00 1 (Speed limit (30km/h))   0.0000000e+00 2 (Speed limit (50km/h))   0.0000000e+00 3 (Speed limit (60km/h))    For the fourth image, the model is 100% sure that it is a \u0026ldquo;Turn right ahead\u0026rdquo;. It is correctly predict this image.\n         Probability Prediction   9.9085110e-01 33 (Turn right ahead)   8.0864038e-03 14 (Stop)   1.0624588e-03 4 (Speed limit (70km/h))   2.1159234e-11 13 (Yield)   7.8768702e-12 2 (Speed limit (50km/h))    Visualizing the model I visulized the output of the first block. The circle and arrows are very clear. ","date":1531724400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531724400,"objectID":"8899ad43543fa956e424fc2c816de8ac","permalink":"http://xincoder.github.io/project/selfdriving_signclassification/","publishdate":"2018-07-16T00:00:00-07:00","relpermalink":"/project/selfdriving_signclassification/","section":"project","summary":"Train a CNN model to recognize traffic signs.","tags":["Self Driving","Python"],"title":"Traffic Sign Recognition [Self Driving]","type":"project"},{"authors":null,"categories":null,"content":" Finding Lane Lines on the Road\nProgramming Language:\n Python  The goal of this project is to design a pipeline that finds lane lines on the road. \nA. Pipeline. My pipline consists of 7 steps as follows:\n Color selection\n RGB image to gray image Gaussian Blur Edge detection (Canny) Select ROI Line detection (Hough) Extend detected lines  This pipeline works on images and videos.\n1. Color selection: Color selection is used to filter color so that we can remove those pixels which may become noise. E.g. patches on the road. The following is a sample of the processed results:\n         Input image Color Selection        One can see that the lines of the current lane have already become very clear and there is no extra noise within the ROI (bottom middle part of the image).\n2.RGB image to gray image: This is a preprocessing for the flowing operations, e.g. edge detection.\n         Color Selection Gray Image        3. Gaussian Blur: This operation is also for edge detection. Using Gaussain Blur helps to remove noise from the current image.\n4. Edge detection (Canny): We detect lines using Canny Transform.\n         Gaussian Blur Edge Detection        5. Select ROI: To remove the background, e.g. side of road or sky, we select a ROI that just in front of the driver (the bottom middle part of the current image).\n         Edge Detection Select ROI        6. Line detection (Hough): We use Hough Transform to detect lines. Instead of drawing the detected lines on the image, I implemented a function (detect_hough_line()) to return all detected lines, so that we can do further process.\n         Select ROI Line Detection        7. Extend detected lines: The lines of a lane consists of solid lines and dashed lines. We want to use a long line to mark the boundaries (left line and right line) of the current lane. Thus: 1. I implemented a function (namely, extend_lines()) to extend the lines, so that all lines can be extended to the bottom of the image and as far as possible. 2. In this function, I also calculate the intersaction of these two lines, so that only the bottom part is marked with lines.\n         Line Detection Extended lines (1)       Calculate intersaction (2) Merge with original image        B. Potential shortcomings of the current pipeline  One potential shortcoming would be caused by the fixed parameters in the current solution, e.g. Gaussian Blur parameters, Edge Detection parameters, Line Detection parameters, and so on. Such manually designed fixed parameters may not work in any application sceneria, e.g. ranning day or night. Another shortcoming would be caused by the first process (color selection). In real world, the color may change in different environment. Thus, in some specific environment, the current pipline may cannot detect any lines or detect too many lines (caused by noise).  C. Suggest possible improvements to this pipeline  For the first optential shortcoming, we would like to come up with an idea to adjust the parameters depening on the environment. Thus, the solution can be robust to the change of the environment. For the second optential shortcoming, we can use the same strategy as the first suggest. By automatically adjusting the paramters of color selection, the method should work. In addition, we can also replace the color selection with other preprocessing methods to avoid the sensitivity of chosen parameters.  ","date":1528786800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528786800,"objectID":"f452cbd96b798f0da7ef3c8049ef6807","permalink":"http://xincoder.github.io/project/selfdriving_findline/","publishdate":"2018-06-12T00:00:00-07:00","relpermalink":"/project/selfdriving_findline/","section":"project","summary":"Design a pipeline that finds lane lines on the road from videos.","tags":["Self Driving","Python"],"title":"Find Lane Lines [Self Driving]","type":"project"},{"authors":null,"categories":null,"content":" In this blog, we are going to use Keras(A Python Deep Learning Library) to implement our deep learning model. It is compatible with Python 2.7-3.5. Keras uses Tensorflow, Theano, or CNTK as its backend engines, so only need to install one of them.\nDependencies:  Keras 2.0.6 (or higher version). Python 2.7-3.5 Tensorflow or Theano or CNTK HDF5 h5py graphviz pydot cuDNN (only for running on GPU) opencv  HDF5 and h5py libraries are used to save our model to disk. graphviz and pydot libraries are needed only when you want to plot model graphs to files (.png or .pdf).\nIn addition, if your computer has one or multiple NVIDIA graph cards, you may want to install cuDNN library to run Keras on GPU.\nNext, I will show you how to setup the environment in Anaconda on MacOS. (For other Operating Systems, please modify the corresponding links or commands.)\n# Open a terminal and type the following commands # Step 1: Download Anaconda \u0026gt; curl -O https://repo.continuum.io/archive/Anaconda3-4.4.0-MacOSX-x86_64.sh # Step 2: Install Anaconda (Use all default settings) \u0026gt; bash Anaconda3-4.4.0-MacOSX-x86_64.sh # Step 3: Restart your terminal # Step 4: Create a virtual environment. (so that it will not mess up the existing settings) \u0026gt; conda create -n keras python=3.5 # Step 5: Install Tensorflow CPU version on Mac \u0026gt; source activate keras \u0026gt; pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.3.0-py3-none-any.whl # If there is an error, please try it again. # Step 6: Install Keras \u0026gt; pip install keras # Step 7: Install other Dependencies \u0026gt; conda install HDF5 \u0026gt; conda install h5py \u0026gt; pip install pydot \u0026gt; pip install graphviz \u0026gt; pip install pillow \u0026gt; conda install -c https://conda.anaconda.org/menpo opencv3 # Step 8: Test \u0026gt; python from keras.models import Sequential from keras.layers import Dense from keras.utils import plot_model model = Sequential() model.add(Dense(10, input_shape=(700, 1))) model.summary() plot_model(model, to_file='abc.pdf', show_shapes=True) exit() # If you get some error like: install graphviz, you can try this command: \u0026gt; brew install graphviz  ","date":1526626800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526626800,"objectID":"ee1a5cd36c0accd42f050a6fee6363e0","permalink":"http://xincoder.github.io/blogs/environment/","publishdate":"2018-05-18T00:00:00-07:00","relpermalink":"/blogs/environment/","section":"blogs","summary":"In this blog, we are going to use Keras(A Python Deep Learning Library) to implement our deep learning model. It is compatible with Python 2.7-3.5. Keras uses Tensorflow, Theano, or CNTK as its backend engines, so only need to install one of them.\nDependencies:  Keras 2.0.6 (or higher version). Python 2.7-3.5 Tensorflow or Theano or CNTK HDF5 h5py graphviz pydot cuDNN (only for running on GPU) opencv  HDF5 and h5py libraries are used to save our model to disk.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" In this blog, we are going to use Neural Network to do image classification. The following figure shows a simple example of Neural Network. If you are interested in this field, please see this review or recently this review. 1. A simple Neural Network on MNIST dataset as an example. The MNIST database is a large database of handwritten digits. It contains 60,000 training images and 10,000 testing images. The Keras provides a convenience method for loading the MNIST dataset.\n# In this demo we design a Neural Network with 1 hidden layers: # Input layer (784 dimensions) # layer 1 (1000 hidden neurons) # layer 2 (10 outputs) import numpy from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense from keras.utils import np_utils # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # flatten 28*28 images to a 784 vector for each image num_pixels = X_train.shape[1] * X_train.shape[2] X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32') X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32') # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test) num_classes = y_test.shape[1] # design model model = Sequential() model.add(Dense(1000, input_dim=num_pixels, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # print out summary of the model print(model.summary()) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) print(\u0026quot;Baseline Error: %.2f%%\u0026quot; % (100-scores[1]*100))  2. Assignment Run the above code to double check your environment and get some sense about how it looks like while training a model. Design your own Neural Network to do Image Classification on Boat Dataset. Boat Dataset consists of 5 different types of boats:\n Training Dataset (249.6 MB) Download\n   Class Number of images     aircraft_carrier 500   banana_boat 500   oil_tanker 500   passenger_ship 500   yacht 500       In total 2500    Testing Dataset (97.1 MB, 1000 images) Download\n  Train your model on training dataset and test the trained model on testing dataset.\nHint Try to understand the following API(s):\n# All images in the new dataset are colorful images (consist of Red, Blue, and Green channels) with different sizes. You may would like to use the following API to resize all images into the same size before doing flatten (line 15 in the above code). image = cv2.imread(path) # load an image resized_image = cv2.resize(image, (new_height, new_width)) # resize an image  To improve the performance of your model, you can try different values of the following parameters:\nnumber of hidden layers (network structure) number of neurons in each layer (network structure) image size (preprocessing) batch_size (training phase, model.compile) optimizer (training phase, model.compile) other settings you can dig out.  3. Submite your sulotion: Your final python code. Please name it using your Lehigh ID. (.py) A short .pdf file. Simply describe what you did, what you got, and other things you want to report, e.g. what you have learned.  ","date":1526626800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526626800,"objectID":"b96153b41b432f4ec3a2a7268c849ce0","permalink":"http://xincoder.github.io/blogs/neural_network/","publishdate":"2018-05-18T00:00:00-07:00","relpermalink":"/blogs/neural_network/","section":"blogs","summary":"In this blog, we are going to use Neural Network to do image classification. The following figure shows a simple example of Neural Network. If you are interested in this field, please see this review or recently this review. 1. A simple Neural Network on MNIST dataset as an example. The MNIST database is a large database of handwritten digits. It contains 60,000 training images and 10,000 testing images. The Keras provides a convenience method for loading the MNIST dataset.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" In this blog, we are going to use Convolutional Neural Network (CNN) to do image classification. The following figure shows the comparison between a 3-layer Neural Network and a simple Convolutional Neural Network. If you are interested in CNN, you can refer this paper which proposes AlexNet. 1. A simple Convolutional Neural Network on MNIST dataset as an example. import numpy from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten from keras.layers.convolutional import Conv2D, MaxPooling2D # load data (X_train, y_train), (X_test, y_test) = mnist.load_data() # reshape to be [samples][pixels][width][height] X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') # normalize inputs from 0-255 to 0-1 X_train = X_train / 255 X_test = X_test / 255 # one hot encode outputs y_train = np_utils.to_categorical(y_train) y_test = np_utils.to_categorical(y_test) num_classes = y_test.shape[1] # create model model = Sequential() model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.2)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(num_classes, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Fit the model model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2) # Final evaluation of the model scores = model.evaluate(X_test, y_test, verbose=0) print(\u0026quot;Baseline Error: %.2f%%\u0026quot; % (100-scores[1]*100))\t 2. Assignment Please run the above code before you design yours. You will notice that using a CNN model gains a higher accuracy than the Neural Netowork on MNIST dataset. Design your own CNN to do Image Classification on Boat Dataset. Boat Dataset consists of 5 different types of boats:\n Training Dataset (249.6 MB) Download\n   Class Number of images     aircraft_carrier 500   banana_boat 500   oil_tanker 500   passenger_ship 500   yacht 500       In total 2500    Testing Dataset (97.1 MB, 1000 images) Download\n  Train your model on training dataset and test the trained model on testing dataset.\nHint\nData augmentation (search how to do data augmentation in Keras) Finetune a pre-trained CNN model. (refer to: keras.applications)  3. Submite your sulotion: Your final python code. Please name it using your Lehigh ID. (.py) A short .pdf file. Simply describe what you did, what you got, and other things you want to report, e.g. what you have learned.  ","date":1526626800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526626800,"objectID":"86df2d9a045ba9935bfaa0a73de24145","permalink":"http://xincoder.github.io/blogs/convolutional_network/","publishdate":"2018-05-18T00:00:00-07:00","relpermalink":"/blogs/convolutional_network/","section":"blogs","summary":"In this blog, we are going to use Convolutional Neural Network (CNN) to do image classification. The following figure shows the comparison between a 3-layer Neural Network and a simple Convolutional Neural Network. If you are interested in CNN, you can refer this paper which proposes AlexNet. 1. A simple Convolutional Neural Network on MNIST dataset as an example. import numpy from keras.datasets import mnist from keras.models import Sequential from keras.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" In this blog, we are going to use LSTMs (Long Short Term Memory Networks) to generate a caption for a given image. LSTMs are a special kind of Recurrent Neural Networks (RNN). If you are looking for some related papers, please refer to paper1 and paper2. The following figure shows the solution of image caption generation proposed in paper2. Next, we will implement a simpler one. The following figure shows the architecture of the implemented model. 1. Dataset We use the flickr30k dataset (4.39 GB) to train an image caption generator. The flickr30k dataset consists of 31,783 images and each one has 5 corresponding captions. We split this dataset into a training subset (21,783 images) and a testing subset (10,000 images).\n2. A simple code Please run the following command first:\n\u0026gt; pip install pillow  Then, the following shows a simple demo code:\nimport os import numpy as np from keras.applications.vgg16 import VGG16, preprocess_input from keras.models import Sequential, Model from keras.layers import LSTM, Dense, Embedding, Merge, Flatten, RepeatVector, TimeDistributed, Concatenate from keras.applications.vgg16 import VGG16, preprocess_input from keras.preprocessing import image as Image from keras.preprocessing import sequence as Sequence from keras.callbacks import TensorBoard, ModelCheckpoint from keras.utils import plot_model, to_categorical from collections import Counter CUDA_VISIBLE_DEVICES='0' os.environ[\u0026quot;CUDA_VISIBLE_DEVICES\u0026quot;] = CUDA_VISIBLE_DEVICES # If you are running on your own computer, please change the following paths to your local paths. # If you are running on HPC, you can keep the following paths. # IMAGE_ROOT = '/Users/xincoder/Documents/Dataset/flickr30k_images/flickr30k_images' # TRAIN_CAPTION_PATH = '/Users/xincoder/Documents/Dataset/flickr30k_images/train.txt' # TEST_CAPTION_PATH = '/Users/xincoder/Documents/Dataset/flickr30k_images/test.txt' IMAGE_ROOT = '/share/ceph/mcc7cse498/xil915/flickr30k_images/flickr30k_images' TRAIN_CAPTION_PATH = '/share/ceph/mcc7cse498/xil915/flickr30k_images/train.txt' TEST_CAPTION_PATH = '/share/ceph/mcc7cse498/xil915/flickr30k_images/test.txt' WORDS_PATH = 'words.txt' SENTENCE_MAX_LENGTH = 100 # In this dataset, the maximum length is 84. EMBEDDING_SIZE = 256 IMAGE_SIZE = 224 CHECK_ROOT = 'checkpoint/' if not os.path.exists(CHECK_ROOT): os.makedirs(CHECK_ROOT) class Data_generator(object): def __init__(self, pra_batch_size=20, pra_word_frequency=2): self.word_frequency = pra_word_frequency # remove words whose frequency less than this value self.train_image_names, self.train_image_captions, self.test_image_names, self.test_image_captions = self.get_name_caption() self.train_image_captions_index = self.caption2index(self.train_image_captions) self.test_image_captions_index = self.caption2index(self.test_image_captions) self.batch_size = pra_batch_size # how many samples we want to train in each step self.train_steps_epoch = len(self.train_image_names)//pra_batch_size # steps per epoch self.test_steps_epoch = len(self.test_image_names)//pra_batch_size # steps per epoch def get_name_caption(self): ''' Load training and testing data from files. We add a \u0026lt;SOS\u0026gt; and \u0026lt;EOS\u0026gt; to the beginning and the end of each sentence respectively. (\u0026lt;SOS\u0026gt; stands for \u0026quot;start of sentence\u0026quot;, \u0026lt;EOS\u0026gt; stands for \u0026quot;end of sentence\u0026quot;) Returns: train_image_name_list: all paths of training images train_caption_list: corresponding training captions test_image_name_list: all paths of testing images test_caption_list: corresponding testing captions ''' with open(TRAIN_CAPTION_PATH, 'r') as reader: content = [x.strip().split('\\t') for x in reader.readlines()] train_image_name_list = [os.path.join(IMAGE_ROOT, x[0].split('#')[0]) for x in content] train_caption_list = ['\u0026lt;SOS\u0026gt; {} \u0026lt;EOS\u0026gt;'.format(x[1].lower()) for x in content] with open(TEST_CAPTION_PATH, 'r') as reader: content = [x.strip().split('\\t') for x in reader.readlines()] test_image_name_list = [os.path.join(IMAGE_ROOT, x[0].split('#')[0]) for x in content] test_caption_list = ['\u0026lt;SOS\u0026gt; {} \u0026lt;EOS\u0026gt;'.format(x[1].lower()) for x in content]\tall_words = ' '.join(train_caption_list+test_caption_list).split(' ') words_num = Counter(all_words) words = [x for x in words_num if words_num[x]\u0026gt;=self.word_frequency] print('{} unique words (all).'.format(len(words_num))) print('{} unique words (count\u0026gt;={}).'.format(len(words), self.word_frequency)) with open(WORDS_PATH, 'w') as writer: writer.write('\\n'.join(words)) return train_image_name_list, train_caption_list, test_image_name_list, test_caption_list def get_dictionary(self, pra_captions): ''' Generate a dictionary for all words in our dataset. Return: words2index: word-\u0026gt;index dictionary index2words: index-\u0026gt;word dictionary ''' if not os.path.exists(WORDS_PATH): words = set(' '.join(pra_captions).split(' ')) with open(WORDS_PATH, 'w') as writer: writer.write('\\n'.join(words)) else: with open(WORDS_PATH, 'r') as reader: words = [x.strip() for x in reader.readlines()] self.voc_size = len(words) words2index = dict((w, ind) for ind, w in enumerate(words, start=0)) index2words = dict((ind, w) for ind, w in enumerate(words, start=0)) return words2index, index2words def caption2index(self, pra_captions): words2index, index2words = self.get_dictionary(pra_captions) captions = [x.split(' ') for x in pra_captions] index_captions = [[words2index[w] for w in cap if w in words2index.keys()] for cap in captions] return index_captions def index2caption(self, pra_index): words2index, index2words = self.get_dictionary('') captions = [' '.join([index2words[w] for w in cap]) for cap in pra_index] return captions\tdef convert2onehot(self, pra_caption): captions = np.zeros((len(pra_caption), self.voc_size)) for ind, cap in enumerate(pra_caption, start=0): captions[ind, cap] = 1 return np.array(captions) def get_epoch_steps(self): return self.train_steps_epoch, self.test_steps_epoch def generate(self, pra_train=True): ''' This is a generator which is used to continuously generate training or testing data. pra_train = True : generate training data pra_train = False : generate testing data ''' while True: if pra_train: # we shuffle training data at the beginning of each epoch. shuffle_index = np.random.permutation(len(self.train_image_names)) image_name_list = np.array(self.train_image_names)[shuffle_index] image_caption_list = np.array(self.train_image_captions)[shuffle_index] image_caption_index_list = np.array(self.train_image_captions_index)[shuffle_index] else: image_name_list = self.test_image_names image_caption_list = self.test_image_captions image_caption_index_list = self.test_image_captions_index image_caption_index_list = Sequence.pad_sequences(image_caption_index_list, maxlen=SENTENCE_MAX_LENGTH, padding='post') input_image_list = [] input_caption_list = [] target_caption_list = [] for index, (image_name, image_caption) in enumerate(zip(image_name_list, image_caption_index_list), start=1): # image input_image = Image.img_to_array(Image.load_img(image_name, target_size=(IMAGE_SIZE, IMAGE_SIZE, 3))) input_caption_onehot = self.convert2onehot(image_caption) target_caption_onehot = np.zeros_like(input_caption_onehot) target_caption_onehot[:-1] = input_caption_onehot[1:] input_image_list.append(input_image) input_caption_list.append(input_caption_onehot) target_caption_list.append(target_caption_onehot) if len(input_image_list) == self.batch_size: tmp_images = np.array(input_image_list) tmp_captions = np.array(input_caption_list) tmp_targets = np.array(target_caption_list) input_image_list = [] input_caption_list = [] target_caption_list = [] yield [preprocess_input(tmp_images), tmp_captions], tmp_targets class Image_Caption(object): def __init__(self, pra_voc_size): self.voc_size = pra_voc_size # Model design start from here. # we use the VGG16 as the base model to extract CNN feature from an image base_model = VGG16(weights='imagenet', include_top=True) base_model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output) for layer in base_model.layers[1:]: layer.trainable = False # add a fully connected layer on the top of our base model # and repeat it several times, so that it has the same shape as our language model image_model = Sequential() image_model.add(base_model) image_model.add(Dense(EMBEDDING_SIZE, activation='relu')) image_model.add(RepeatVector(SENTENCE_MAX_LENGTH)) # we use an Embedding layer to generate a good representation for captions. language_model = Sequential() # language_model.add(Embedding(self.voc_size, EMBEDDING_SIZE, input_length=SENTENCE_MAX_LENGTH)) language_model.add(LSTM(128, input_shape=(SENTENCE_MAX_LENGTH, self.voc_size), return_sequences=True)) language_model.add(TimeDistributed(Dense(128))) # after merging CNN feature (image) and embedded vector (caption), we feed them into a LSTM model # at its end, we use a fully connected layer with softmax activation to convert the output into probability model = Sequential() model.add(Merge([image_model, language_model], mode='concat')) # model.add(Concatenate([image_model, language_model])) model.add(LSTM(1000, return_sequences=True)) # model.add(Dense(self.voc_size, activation='softmax', name='final_output')) model.add(TimeDistributed(Dense(self.voc_size, activation='softmax'))) # draw the model and save it to a file. # plot_model(model, to_file='model.pdf', show_shapes=True) self.model = model self.model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) def train_model(self, pra_datagen): # callback: draw curve on TensorBoard tensorboard = TensorBoard(log_dir='log', histogram_freq=0, write_graph=True, write_images=True) # callback: save the weight with the highest validation accuracy filepath=os.path.join(CHECK_ROOT, 'weights-improvement-{val_acc:.4f}-{epoch:04d}.hdf5') checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=2, save_best_only=True, mode='max') # train model self.model.fit_generator( pra_datagen.generate(True), steps_per_epoch=pra_datagen.get_epoch_steps()[0], epochs=5, validation_data=pra_datagen.generate(False), validation_steps=pra_datagen.get_epoch_steps()[1], callbacks=[tensorboard, checkpoint]) if __name__ == '__main__': my_generator = Data_generator() model = Image_Caption(my_generator.voc_size) model.train_model(my_generator) # for [img, cap], tar in my_generator.generate(): # print(img.shape, cap.shape, tar.shape) # print(np.argmax(cap[0, 0]), np.argmax(tar[0, 0])) # print(np.argmax(cap[0, 1]), np.argmax(tar[0, 0])) # print('')  2. Assignment Run the above code to train the model and use it as a baseline. Modify the above code to improve the accuracy. Add a function to generate a sentence for a given image.  Hint: To improve the accuracy, you can try: - different base models. (refer to: keras.applications) - different RNNs. (refer to: keras.layers.recurrent) - data augmentation by randomly rotating, fliping, or shifting training images.\n3. Submite your sulotion: Your final python code. Please name it using your Lehigh ID. (.py) A short .pdf file. Simply describe what you did, what you got, and other things you want to report, e.g. what you have learned.  4. Train your model on HPC It will take weeks (or months) if you only use the CPU on your laptops to train the model. Considering this, Prof. Chuah has already applied Lehigh University Research Computing (HPC) resource for all of you. (You may have already received an email from root@polaris.cc.lehigh.edu). Please run your code on your own computer first to make sure that there is no error before you run it on HPC.\nYou can access HPC via SSH. - For Windows users: please download Putty. - For Mac users: you can use SSH in a terminal.\nThe username and password for HPC is your LehighID and the corresponding password. For example, my LehighID id xil915, then I can access HPC using the following command:\nssh \u0026lt;your_LehighID\u0026gt;@sol.cc.lehigh.edu  All training and testing data have been saved in a shared directory:\n/share/ceph/.../.../flickr30k_images  Once you login, you need to create two files in your own directories: - your python code, namely image_caption.py. - a bash file, namely run.sh\nSave the following script into your run.sh:\n#!/bin/tcsh #SBATCH --partition=imlab-gpu #SBATCH --time=100:00:00 # maximum time #SBATCH --nodes=1 # 1 CPU can be be paired with only 1 GPU #SBATCH --ntasks-per-node=1 #SBATCH --gres=gpu:1 # Need both GPUs, use --gres=gpu:2 #SBATCH --job-name xin_image_caption #SBATCH --output=\u0026quot;log.%j.%N.out\u0026quot; module load python/cse498 python image_caption.py  Run your code:\nsbatch run.sh  This command will sumbit your job to a waitinglist. Please use the following command to check the status of your jobs:\n\u0026gt; squeue # list all jobs \u0026gt; squeue -u xil915 # your LehighID, only list your job(s).  This is my output:\n[xil915@sol CSE498]$ squeue -u xil915 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 187364 imlab-gpu xin_imag xil915 PD 0:00 1 (Priority) # ST == PD : this job is pending # ST == R : this job is running  Cancel your job using:\n\u0026gt; scancel 187364 # JOBID  When your job is running, all standard outputs will be saved in to file namely, log.*.out (e.g. log.187364.sol-b411.out). You can print it out using:\n\u0026gt; cat log.187364.sol-b411.out  HELP !!! Considering that the long waiting list on HPC, I provide a pre-trained model using my demo code to you.\nDuring your model is training, you can use my pre-trained model to test your implemented function. (generating a sentence for a given image)\nDownloads: pre-trained model, words index\n The pre-trained model only was trained for 2 epochs. The words index is a file which list the indices of words. (A good result is not expected using this pre-trained model, considering that this is only trained for 2 epochs.)  Load weights:\nmodel.load_weights(filepath)  Please note it in your report, if you use this pre-trained model to generate the final results.\n","date":1526626800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526626800,"objectID":"e91770b9c00c4452fef583e5151560b4","permalink":"http://xincoder.github.io/blogs/image_caption/","publishdate":"2018-05-18T00:00:00-07:00","relpermalink":"/blogs/image_caption/","section":"blogs","summary":"In this blog, we are going to use LSTMs (Long Short Term Memory Networks) to generate a caption for a given image. LSTMs are a special kind of Recurrent Neural Networks (RNN). If you are looking for some related papers, please refer to paper1 and paper2. The following figure shows the solution of image caption generation proposed in paper2. Next, we will implement a simpler one. The following figure shows the architecture of the implemented model.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" How to train an object classifier using our own images 1. Info: I prepared two python scripts (retrain.py, predict.py) for this task.\n retrain.py: used to train the classifier. predict.py: used to load the trained model and test on new images.  2. Prepare training and testing data:  Training Data: Let\u0026rsquo;s assume that we have two classes, namely \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;dog\u0026rdquo;. We just need to make sure that there are two sub-folders in \u0026ldquo;training_images\u0026rdquo; folder. Each sub-folder only consists of its own images.\n|- training_images |- cat |- image_0.jpg |- image_1.jpg ... |- dog |- image_2.jpg |- image_3.jpg ...  Testing Data: All testing images are put in one folder, e.g. \u0026ldquo;testing_images\u0026rdquo;.\n|- testing_images |- image_0.jpg |- image_1.jpg |- image_2.jpg |- image_3.jpg ...   3. Train a classifier Assume that both \u0026ldquo;training_images\u0026rdquo; and \u0026ldquo;testing_images\u0026rdquo; are in this folder:\n\u0026gt; ls models/ predict.py retrain.py testing_images/ training_images/  Then, we can type the following command to starting training process:\n\u0026gt; python retrain.py \\ --bottleneck_dir=./bottlenecks \\ --how_many_training_steps=50000 \\ --model_dir=./inception \\ --output_graph=./models/retrained_graph.pb \\ --output_labels=./models/retrained_labels.txt \\ --summaries_dir=./retrain_logs \\ --validation_batch_size=5000 \\ --image_dir=training_images # this is the folder of training data  After this training process finished, it saves the trained model in \u0026ldquo;./models\u0026rdquo;.\n\u0026gt; ls models retrained_graph.pb retrained_labels.txt  4. Predict new images using trained model Type the following command in a terminal to run the testing code:\n\u0026gt; python predict.py \\ --models_folder='./models' \\ --test_image_folder='./testing_images' \\ --display_image=False  ","date":1526626800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526626800,"objectID":"fe5afde5928a4a07a3b1684386f0378b","permalink":"http://xincoder.github.io/blogs/train_classifier/","publishdate":"2018-05-18T00:00:00-07:00","relpermalink":"/blogs/train_classifier/","section":"blogs","summary":"How to train an object classifier using our own images 1. Info: I prepared two python scripts (retrain.py, predict.py) for this task.\n retrain.py: used to train the classifier. predict.py: used to load the trained model and test on new images.  2. Prepare training and testing data:  Training Data: Let\u0026rsquo;s assume that we have two classes, namely \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;dog\u0026rdquo;. We just need to make sure that there are two sub-folders in \u0026ldquo;training_images\u0026rdquo; folder.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" How to use a pre-trained mode on an Android device In previous chapter, we discussed how to train an object classifier using our own images. At the end, we got trained model and labels file (retrained_graph.pb, retrained_labels.txt).\nIn this chapter, We are going to load pre-trained classifer in our Android app. Unfortunately, we can not use the trained model on Android directly. We need to optimize it using a tool, namely \u0026ldquo;optimize_for_inference\u0026rdquo;, provided by Tensorflow.\n1. Build tool \u0026ldquo;optimize_for_inference\u0026rdquo;  Download Tensorflow source code:\ngit clone https://github.com/tensorflow/tensorflow.git  Install bazel so that we can use it to build \u0026ldquo;optimize_for_inference\u0026rdquo;\necho \u0026quot;deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\u0026quot; | sudo tee /etc/apt/sources.list.d/bazel.list curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install bazel sudo apt-get upgrade bazel  Build \u0026ldquo;optimize_for_inference\u0026rdquo;\ncd tensorflow ./configure # We can choose all default settings bazel build tensorflow/python/tools:optimize_for_inference # this process takes a while, be patient    2. Optimize trained model Let\u0026rsquo;s assume that our pre-trained model is \u0026lt;folder_path\u0026gt;/retrained_graph.pb. Then, we can use the following command to optimize the model and save it as \u0026lt;folder_path\u0026gt;/retrained_graph_android.pb\nbazel-bin/tensorflow/python/tools/optimize_for_inference \\ --input=\u0026lt;folder_path\u0026gt;/retrained_graph.pb \\ --output=\u0026lt;folder_path\u0026gt;/retrained_graph_android.pb \\ --input_names=Mul \\ --output_names=final_result  3. Modify Tensorflow Android Demo  Download Android Demo:\ngit clone https://github.com/Nilhcem/tensorflow-classifier-android.git  If we import this project into Android Studio, compile and run, the demo will load a pre-trained classifier which can recognize 1000 classes.\n Load our own model:\n Delete the previous ImageNet model from assets/ folder. Copy our optimized trained model retrained_graph_android.pb and label file retrained_labels.txt into assets/ folder. Open ClassifierActivity.java and set the following variables:\nprivate static final int INPUT_SIZE = 299; private static final int IMAGE_MEAN = 128; private static final float IMAGE_STD = 128; private static final String INPUT_NAME = \u0026quot;Mul\u0026quot;; private static final String OUTPUT_NAME = \u0026quot;final_result\u0026quot;;  Compile and run. The demo will open the camera and show the confidence score of each corresponding class.\n   ","date":1526626800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526626800,"objectID":"a5dde2a2b902a3c7988d6e1e8fa84ad2","permalink":"http://xincoder.github.io/blogs/model_on_android/","publishdate":"2018-05-18T00:00:00-07:00","relpermalink":"/blogs/model_on_android/","section":"blogs","summary":"How to use a pre-trained mode on an Android device In previous chapter, we discussed how to train an object classifier using our own images. At the end, we got trained model and labels file (retrained_graph.pb, retrained_labels.txt).\nIn this chapter, We are going to load pre-trained classifer in our Android app. Unfortunately, we can not use the trained model on Android directly. We need to optimize it using a tool, namely \u0026ldquo;optimize_for_inference\u0026rdquo;, provided by Tensorflow.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" In this chapter, I list all materials related to the trained classifier which is used to classify an image into 5 different types of boats:\n iceboat shrimper patrol boat fishing boat weather ship  0. Trained model The trained model consists of two files:\n retrained_graph.pb retrained_labels.txt  1. Downloaded Dataset  Training dataset (475.6 MB) Download\n   Class Number of images     iceboat 1692   shrimper 2380   patrol_boat 1820   fishing_boat 1320   weather_ship 1101       In total 8313    Testing dataset (34.7 MB, 1532 images) Download\n  2. Test trained model After we downloaded the trained model and Testing dataset, can use this code to test this model by type the following command in a terminal:\n\u0026gt; python predict.py \\ --models_folder='./models' \\ --test_image_folder='./testing_images' \\ --display_image=False  3. Use trained model on Android This android demo loads our trained boat classifier and classify the camera video frame. - Android apk file (103.4 MB): Xin_Boat_Demo.apk (I only tested this app on our ASUS NEXUS 7 Tablet.) - Source Code (392.1 MB): tensorflow-classifier-android.zip (already contained trained model)\nThis figure is the screenshot: If you do not want to know more details about how did I collect the data, you can skip the following part of this chapter. Image Collection  All training images are downloaded from ImageNet. In order to make it easier, I wrote a python script (download_image.py) to download images automatically. (PS: some time the it takes a long time to access ImageNet, which causes a time out error of the script. We need to login the ImageNet (easy to register an account) in our browser first.) There are some links not accessable any more, which results in some downloaded files are not images. Thus, I wrote another python script (remove.py) to remove them. In addition, some images are error message and we need to manually remove them. I used this tool to augment the training images.  ","date":1526626800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526626800,"objectID":"61737259d9c1501901941e0bc4d79330","permalink":"http://xincoder.github.io/blogs/boat_classifier/","publishdate":"2018-05-18T00:00:00-07:00","relpermalink":"/blogs/boat_classifier/","section":"blogs","summary":"In this chapter, I list all materials related to the trained classifier which is used to classify an image into 5 different types of boats:\n iceboat shrimper patrol boat fishing boat weather ship  0. Trained model The trained model consists of two files:\n retrained_graph.pb retrained_labels.txt  1. Downloaded Dataset  Training dataset (475.6 MB) Download\n   Class Number of images     iceboat 1692   shrimper 2380   patrol_boat 1820   fishing_boat 1320   weather_ship 1101       In total 8313    Testing dataset (34.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":"I built a server with 2 Nvidia Titan Xp for my research. The following table shows the details of my own sever:\n   Hardware Detail Quantity Price     CPU Intel - Xeon E5-2630 V4 2.2GHz 10-Core Processor 1 $679.99   Liquid CPU Cooler NZXT - Kraken X62 Liquid CPU Cooler 1 $156.29   Memory Corsair - Vengeance LPX 32GB (2 x 16GB) DDR4-2400 Memory 2 $347.02   MotherBoard Asus - ROG STRIX X99 GAMING ATX LGA2011-3 Motherboard 1 $306.74   Graphics Card NVIDIA GeForce Titan X Pascal 12GB GDDR5X 2 $2,400.00   SLI Bridge ASUS ROG SLI High-Bandwidth Bridge with Aura Sync RGB 3 Slot (ROG-SLI-HB-BRIDGE-3SLOT) 1 $49.99   SSD Intel 535 Series 240GB SATA III 6Gb/s 2.5 Solid State Drive 2 $69.99   SSD Samsung 850 EVO 1TB 2.5-Inch SATA III Internal SSD (MZ-75E1T0B/AM) 2 $599.98   Power Supply Corsair HXi Series, HX1000i 1000 Watt Fully Modular Power Supply 1 $219.99   Case Phanteks - Enthoo Evolv ATX ATX Mid Tower Case 1 $175.29   Keyboard \u0026amp; Mouse Microsoft Wireless Desktop 900 Keyboard \u0026amp; Mouse Combo 1 $29.99   Monitor Dell UltraSharp U2414H 24-Inch Screen LED Monitor 1 $214.72   Wireless Adapter Panda Wireless PAU06 300Mbps N USB Adapter 1 $19.99   UPS APC - SMT1500 UPS 1 $419.00         Before Tax:   $5,688.98   Tax:   $341.34   In Total:   $6,030.32    ","date":1526454000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526454000,"objectID":"2bf920cad3f09599a58221daeaf84332","permalink":"http://xincoder.github.io/blogs/panda_list/","publishdate":"2018-05-16T00:00:00-07:00","relpermalink":"/blogs/panda_list/","section":"blogs","summary":"I built a server with 2 Nvidia Titan Xp for my research. The following table shows the details of my own sever:\n   Hardware Detail Quantity Price     CPU Intel - Xeon E5-2630 V4 2.2GHz 10-Core Processor 1 $679.99   Liquid CPU Cooler NZXT - Kraken X62 Liquid CPU Cooler 1 $156.29   Memory Corsair - Vengeance LPX 32GB (2 x 16GB) DDR4-2400 Memory 2 $347.","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" After building a sever, I did the following operations to setup the environment.\n1. Create a USB ubuntu Installer (on Mac) Create a bootable USB on MacOS, so that we can use it to install Ubuntu 16.04.\n# Eject the USB from your MAC cd ~/Downloads # rename the ISO file with a shorter name hdiutil convert -format UDRW -o ubuntu.iso ubuntu-16.04.3-desktop-amd64.iso mv ubuntu.iso.dmg ubuntu.iso diskutil list # Plug in the USB and figure out the disk ID. diskutil list diskutil unmountDisk /dev/disk3 # disk3 is my USB sudo dd if=./ubuntu.iso of=/dev/rdisk3 bs=1m # disk3 is my USB  2. Install CUDA Download Nvidia display driver from Nvidia. (NVIDIA-Linux_*.run) Download CUDA \u0026ldquo;runfile\u0026rdquo; from Nvidia.\n# Install SSH server sudo apt-get install openssh-server # Turn off the lightdm sudo service lightdm stop sudo apt-get install vim sudo apt-get install dkms build-essential linux-headers-generic sudo vi /etc/modprobe.d/blacklist.conf # insert the following lines to the end: blacklist nouveau #blacklist lbm-nouveau #options nouveau modeset=0 #alias nouveau off #alias lbm-nouveau off #\u0026gt; echo options nouveau modeset=0 | sudo tee -a /etc/modprobe.d/nouveau-kms.conf sudo update-initramfs -u sudo reboot sudo service lightdm stop sudo ./NVIDIA-Linux-x86_64-390.48.run --no-x-check --no-nouveau-check --no-opengl-files sudo reboot sudo ./cuda_9.0.176_384.81_linux.run --no-opengl-libs # This time, do not install driver, do not choose opengl and X configuration vi ~/.bashrc # insert the following lines to the end: export LD_LIBRARY_PATH=\u0026quot;/usr/local/cuda-8.0/lib64/\u0026quot; export CUDA_BIN=/usr/local/cuda-8.0/bin export CUDA_LIB=/usr/local/cuda-8.0/lib64 export PATH=${CUDA_BIN}:$PATH # \u0026gt; sudo reboot  3. Enable Nvidia Driver After reboot, open \u0026ldquo;Additional Drivers\u0026rdquo; and choose \u0026ldquo;Using Nvidia binary driver\u0026rdquo;.\n4. Install CuDNN Download CuDNN 6.0.\ntar -zxvf cudnn-8.0-linux-x64-v6.0tgz cd cuda sudo cp lib* /usr/local/cuda-8.0/lib64/ sudo cp cudnn.h /usr/local/cuda-8.0/include/ cd /usr/local/cuda-8.0/lib64 # update links sudo rm libcudnn.so libcudnn.so.6 sudo ln -s libcudnn.so.6.0.21 libcudnn.so.6 sudo ln -s libcudnn.so.6 libcudnn.so  5. Install Keras # Step 1: Download Anaconda # On Linux wget https://repo.continuum.io/archive/Anaconda3-4.4.0-Linux-x86_64.sh # On MacOS curl -O https://repo.continuum.io/archive/Anaconda3-4.4.0-MacOSX-x86_64.sh # Step 2: Install Anaconda (Use all default settings) bash Anaconda3-4.4.0-MacOSX-x86_64.sh # Step 3: Restart your terminal # Step 4: Create a virtual environment. (so that it will not mess up the existing settings) conda create -n keras python=3.5 # Step 5: Install Tensorflow source activate keras # GPU version on Linux pip install tensorflow-gpu # CPU version on Mac pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.3.0-py3-none-any.whl # If there is an error, please try it again. # Step 6: Install Keras # on MacOs brew install graphviz # On Linux sudo apt-get install python-pydot python-pydot-ng graphviz pip install keras # Step 7: Install other Dependencies conda install HDF5 conda install h5py pip install pydot pip install graphviz pip install pillow pip install opencv-python # conda install -c https://conda.anaconda.org/menpo opencv3 # for visualize the model pip install quiver_engine pip install keras-vis # Step 8: Test python  from keras.models import Sequential from keras.layers import Dense from keras.utils import plot_model model = Sequential() model.add(Dense(10, input_shape=(700, 1))) model.summary() plot_model(model, to_file='abc.pdf', show_shapes=True) exit()  6. Install other dependencies # moviepy pip install moviepy # If there is a ffmpeg error, please add the following lines at the top of your python file: import imageio imageio.plugins.ffmpeg.download() # ERROR # This error can ben due to the fact that ImageMagick is not installed on your computer. # on ubuntu sudo apt-get install libmagickwand-dev # on mac brew install imagemagick sudo vi /etc/ImageMagick-6/policy.xml comment: \u0026lt;policy domain=\u0026quot;path\u0026quot; rights=\u0026quot;none\u0026quot; pattern=\u0026quot;@*\u0026quot; /\u0026gt; # to ensure TextClip works brew install imagemagick # BeautifulSoup4 | bs4 pip install BeautifulSoup4  7. Install Sublime # On Linux (need to pay?) wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add - echo \u0026quot;deb https://download.sublimetext.com/ apt/stable/\u0026quot; | sudo tee /etc/apt/sources.list.d/sublime-text.list # OR echo \u0026quot;deb https://download.sublimetext.com/ apt/dev/\u0026quot; | sudo tee /etc/apt/sources.list.d/sublime-text.list sudo apt-get update sudo apt-get install sublime-text  8. Setup LogmeIn wget http://www.vpn.net/installers/logmein-hamachi_2.1.0.165-1_amd64.deb sudo dpkg -i logmein-hamachi_2.1.0.165-1_amd64.deb sudo hamachi login sudo hamachi attach ***@gmail.com sudo hamachi create \u0026lt;server_name\u0026gt; \u0026lt;password\u0026gt;  9. Setup OpenAI conda create -n openai python=3.5 source activate openai pip install gym # if you want to use Breakout-ram-v0, please install atari pip install gym[atari] # Install tensorflow, keras, and keras-rl pip install tensorflow pip install keras pip install keras-rl  Then, you can try the following code:\nimport numpy as np import gym from keras.models import Sequential from keras.layers import Dense, Activation, Flatten from keras.optimizers import Adam from rl.agents import DDPGAgent, SARSAAgent, DQNAgent from rl.policy import BoltzmannQPolicy from rl.memory import SequentialMemory ENV_NAME = 'Breakout-ram-v0' # Get the environment and extract the number of actions. env = gym.make(ENV_NAME) np.random.seed(123) env.seed(123) nb_actions = env.action_space.n # Next, we build a very simple model. model = Sequential() model.add(Flatten(input_shape=(1,) + env.observation_space.shape)) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(16)) model.add(Activation('relu')) model.add(Dense(nb_actions)) model.add(Activation('linear')) print(model.summary()) # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and even the metrics! memory = SequentialMemory(limit=50000, window_length=1) policy = BoltzmannQPolicy() dqn = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=10, policy=policy) dqn.compile(Adam(lr=1e-3), metrics=['mae']) # Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. You can always safely abort the training prematurely using Ctrl + C. dqn.fit(env, nb_steps=50000, visualize=True, verbose=2) # After training is done, we save the final weights. dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True) # Finally, evaluate our algorithm for 5 episodes. dqn.test(env, nb_episodes=5, visualize=True)  pip install tensorflow keras conda install HDF5 h5py pip install pydot graphviz pillow quiver_engine keras-vis opencv-python moviepy BeautifulSoup4  pip install mxnet-cu80mkl  Automatically mount HD/SSD  Type the following command to get the UUID of your HD/SSD.\nsudo blkid  The output looks similar as the follows:\n/dev/sda2: LABEL=\u0026quot;Data\u0026quot; UUID=\u0026quot;1438008138******\u0026quot; TYPE=\u0026quot;ntfs\u0026quot; PARTLABEL=\u0026quot;Basic data partition\u0026quot; PARTUUID=\u0026quot;39e88529-****-****-****-************\u0026quot; /dev/sdb2: LABEL=\u0026quot;BigData\u0026quot; UUID=\u0026quot;3868FD7668******\u0026quot; TYPE=\u0026quot;ntfs\u0026quot; PARTLABEL=\u0026quot;Basic data partition\u0026quot; PARTUUID=\u0026quot;dec90a31-****-****-****-************\u0026quot; /dev/sdc1: UUID=\u0026quot;4a76c292-3384-4480-9672-4c8ab9******\u0026quot; TYPE=\u0026quot;swap\u0026quot; PARTUUID=\u0026quot;7bd24d17-****-****-****-************\u0026quot; /dev/sdc2: UUID=\u0026quot;447c124c-baf5-4a24-a2c7-ac413f******\u0026quot; TYPE=\u0026quot;ext4\u0026quot; PARTUUID=\u0026quot;a96835ae-****-****-****-************\u0026quot; /dev/sdc3: UUID=\u0026quot;88bd1683-c1fd-4097-b3c9-71998d******\u0026quot; TYPE=\u0026quot;ext4\u0026quot; PARTUUID=\u0026quot;7184ca16-****-****-****-************\u0026quot; /dev/sda1: PARTLABEL=\u0026quot;Microsoft reserved partition\u0026quot; PARTUUID=\u0026quot;d52eb0c9-****-****-****-************\u0026quot; /dev/sdb1: PARTLABEL=\u0026quot;Microsoft reserved partition\u0026quot; PARTUUID=\u0026quot;bc7c1633-****-****-****-************\u0026quot;  Add the corresponding info. to /etc/fstab\nsudo vi /etc/fstab # add the following info. to the end of the file UUID=1438008138****** /data/Data ntfs defaults 0 2 UUID=3868FD7668****** /data/BigData ntfs defaults 0 2  It will automatically mount these two HDs/SSDs at the next boot.\n  Use remote jupyter on local ssh -L 8000:localhost:8888 username@IP jupyter notebook --no-browser --port=8889 ssh -N -L localhost:8888:localhost:8889 user@remote_host ### list ssh ps aux | grep ssh  ","date":1526454000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526454000,"objectID":"73533274d83fb02326d6b8b7ca008031","permalink":"http://xincoder.github.io/blogs/setup_environment/","publishdate":"2018-05-16T00:00:00-07:00","relpermalink":"/blogs/setup_environment/","section":"blogs","summary":"After building a sever, I did the following operations to setup the environment.\n1. Create a USB ubuntu Installer (on Mac) Create a bootable USB on MacOS, so that we can use it to install Ubuntu 16.04.\n# Eject the USB from your MAC cd ~/Downloads # rename the ISO file with a shorter name hdiutil convert -format UDRW -o ubuntu.iso ubuntu-16.04.3-desktop-amd64.iso mv ubuntu.iso.dmg ubuntu.iso diskutil list # Plug in the USB and figure out the disk ID.","tags":null,"title":"","type":"docs"},{"authors":["**Xin Li**","Mooi Choo Chuah"],"categories":null,"content":"","date":1520838000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520838000,"objectID":"ba6bc4d22603268e02119faa26f7da9f","permalink":"http://xincoder.github.io/publication/2018wacv_rehar/","publishdate":"2018-03-12T00:00:00-07:00","relpermalink":"/publication/2018wacv_rehar/","section":"publication","summary":"","tags":null,"title":"ReHAR: Robust and Efficient Human Activity Recognition","type":"publication"},{"authors":["**Xin Li**","Mooi Choo Chuah"],"categories":null,"content":"","date":1508655600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508655600,"objectID":"6a234f1463055ce74ee71f3ae67e7712","permalink":"http://xincoder.github.io/publication/2017iccv_sbgar/","publishdate":"2017-10-22T00:00:00-07:00","relpermalink":"/publication/2017iccv_sbgar/","section":"publication","summary":"","tags":null,"title":"SBGAR: Semantics based group activity recognition","type":"publication"},{"authors":["**Xin Li**","Mooi Choo Chuah","Subhrajit Bhattacharya"],"categories":null,"content":"","date":1497337200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497337200,"objectID":"0c2ee7b5f1636f1df2e90022a2be80c0","permalink":"http://xincoder.github.io/publication/2017icuas_uav/","publishdate":"2017-06-13T00:00:00-07:00","relpermalink":"/publication/2017icuas_uav/","section":"publication","summary":"","tags":null,"title":"UAV assisted smart parking solution","type":"publication"},{"authors":null,"categories":null,"content":"Programming Language:\n Python Java (Android Client) PHP (Web Interface)  This is a prototype of our CASHEIRS image retrieval system. The initial prototype system consists of a cloud server (a laptop) and a Samsung S5 phone as a data user\u0026rsquo;s mobile device. Samsung S5 has a Snapdragon 801 chip with Quad-core CPU@2.5GHz and 2GB RAM. The smartphone communicates with the server via a WiFi router. In the Android client software, Caffe APIs are called to extract CNN feature from a query image. Then, this CNN feature is converted into 128bit binary code and later encrypted. To handle complex image transformations and minimize memory usage, Picasso, an image downloading library for Android, is used to download query results.\nThe following images show some query examples.\n                     My responsibilities:\n Proposed and designed an image retrieval system. Implemeted the system including an Android Client, a Python Server and a PHP interface.  Published paper:\n Xin Li, Qinghan Xue, Mooi Choo Chuah (2017). CASHEIRS: Cloud assisted scalable hierarchical encrypted based image retrieval system. INFOCOM 2017-IEEE Conference on Computer Communications (INFOCOM).   ","date":1493622000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493622000,"objectID":"2518be16d2997d38f2245f2e9ef53af6","permalink":"http://xincoder.github.io/project/research_casheirs/","publishdate":"2017-05-01T00:00:00-07:00","relpermalink":"/project/research_casheirs/","section":"project","summary":"This is prototype of our CASHEIRS image retrieval system.","tags":["Research Project","Python","Android","PHP"],"title":"CASHEIRS Demo [Research]","type":"project"},{"authors":["**Xin Li**","Qinghan Xue","Mooi Choo Chuah"],"categories":null,"content":"","date":1493622000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493622000,"objectID":"e5bdc911a6122f8af395c7d6c0f4bc14","permalink":"http://xincoder.github.io/publication/2017infocom_casheirs/","publishdate":"2017-05-01T00:00:00-07:00","relpermalink":"/publication/2017infocom_casheirs/","section":"publication","summary":"","tags":null,"title":"CASHEIRS: Cloud assisted scalable hierarchical encrypted based image retrieval system","type":"publication"},{"authors":null,"categories":null,"content":" Detection and Description of Visual Attributes for Vehicles and Pedestria Programming Language:\n Python (Server) HTML5 (Interface) JaveScript  In this project, we designed and implemented a very simple AI for the Five-in-a-Row Game. Black plays first if white did not just win, and players alternate in placing a stone of their color on an empty intersection. The winner is the first player to get an unbroken row of five stones horizontally, vertically, or diagonally.\nMy responsibilities:\n Designed and implemented a JaveScript client embedded inside an HTML5 webpage that allows two players to play five-in-a-row game. Designed and implemented a server application that judges which player wins in the end. Implemented a Python AI program that can play against a single player. The AI program at least beats a player that randomly place stones on the board.  The following video shows a demo of our system.\n ","date":1446710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446710400,"objectID":"22ee8c2e563fb8a83aeb3b09ebdcf921","permalink":"http://xincoder.github.io/project/others_fivestones/","publishdate":"2015-11-05T00:00:00-08:00","relpermalink":"/project/others_fivestones/","section":"project","summary":"This is a demo of my very simple AI for Five-in-a-Row Game.","tags":["Others","Python"],"title":"Five-in-a-Row Game [Others]","type":"project"},{"authors":null,"categories":null,"content":" Detection and Description of Visual Attributes for Vehicles and Pedestria Programming Language:\n C++ SQL  In this project, we designed and implemented a system to detect and track vehicles and pedestrains from traffic surveillance videos. Once a vehicle/pedestrain is detected/tracked, some visual descriptions, e.g. color, moving direction, etc., are analyzed. All information are stored into a local SQL database for retrieval in the future.\nMy responsibilities:\n Proposed and implemented a detection system for vehicles. Implemeted a tracking function. Extract visual descriptions from detected vehicles. Implemented objects retrieval functions.   The following video shows a demo of our system.\n ","date":1433142000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1433142000,"objectID":"2a9a17fc97437c3f95417276eecc9bdd","permalink":"http://xincoder.github.io/project/research_vehicle_pedestrian/","publishdate":"2015-06-01T00:00:00-07:00","relpermalink":"/project/research_vehicle_pedestrian/","section":"project","summary":"A prototype system.","tags":["Research Project","Cpp","SQL"],"title":"Vehicles and Pedestrians Detection [Research]","type":"project"},{"authors":null,"categories":null,"content":" Detection and Description of Visual Attributes for Vehicles and Pedestria Programming Language:\n C++ (PC server: Object and Motion Detection, Alarm module) Java (Android Client) PHP (Web Interface)  In this project, we designed an embedded family video surveillance system. The system consists of two parts: (1) a PC server and (2)an Android client.\nThe PC server can acquire, compress, and process camera frames. Once the PC server detected an abnormal condition, e.g. a moving people, it will send an alarm to the Android client.\nThe Android client can automatically receive the alarm information and download the corresponding video by on click.\nMy responsibilities:\n Designed a video survellance system that consists of ARM development board (camera), a PC server, and an Android client. Implemented the main functions of the system: people detection, motion detection, alarm module. Implemented an Android app to recieve the alarm from the server and download videos.  The following video shows a demo of our system.\n ","date":1378018800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1378018800,"objectID":"59c13409d29d1b52ee89604c57f37d6c","permalink":"http://xincoder.github.io/project/others_family_surveillance/","publishdate":"2013-09-01T00:00:00-07:00","relpermalink":"/project/others_family_surveillance/","section":"project","summary":"A prototype system for our surveillance system.","tags":["Others","Cpp","Android","PHP"],"title":"Family Video Surveillance System [Others]","type":"project"},{"authors":null,"categories":null,"content":" Detection and Description of Visual Attributes for Vehicles and Pedestria Programming Language:\n C# (Unity) Unity Script (Unity)  In this project, we designed and implemented an augmented reality system based on tablet for Autodesk Maya. The app runs on an Android tablet records the trajectory of the virtual camera. In this way, movie directors do not need to manually set the trajectory of the virtual camera in Maya.\nMy responsibilities:\n Developed an Android app to interactively record the trajectory of the virtual camera based on the device sensors\u0026rsquo; data. Implemented the function of replaying the trajectory based on the recorded data.  The following video shows a demo of our system.\n ","date":1375340400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375340400,"objectID":"bb160bd29ed19aa6a9088a0c149dba5b","permalink":"http://xincoder.github.io/project/others_ar/","publishdate":"2013-08-01T00:00:00-07:00","relpermalink":"/project/others_ar/","section":"project","summary":"A demo of our Augmented Reality Cinema Project.","tags":["Others","Android","Unity"],"title":"Augmented Reality Cinema [Others]","type":"project"}]